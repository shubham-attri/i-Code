{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ContentList'></a>\n",
    "# Content List\n",
    "\n",
    "## Single to Single Generation \n",
    "\n",
    "### 1. [Text To Image](#TextToImage)\n",
    "\n",
    "### 2. [Image To Text](#ImageToText)\n",
    "\n",
    "### 3. [Text To Audio](#TextToAudio)\n",
    "\n",
    "### 4. [Audio To Text](#AudioToText)\n",
    "\n",
    "### 5. [Image To Audio](#ImageToAudio)\n",
    "\n",
    "### 6. [Audio To Image](#AudioToImage)\n",
    "\n",
    "### 7. [Text To Video](#TextToVideo)\n",
    "\n",
    "## Multi-Conditioning Generation\n",
    "\n",
    "### 1. [Text + Image + Audio To Image](#TextImageAudioToImage)\n",
    "\n",
    "## Joint Multimodal Generation\n",
    "\n",
    "### 1. [Text To Image+Text](#TextToImageText)\n",
    "\n",
    "### 2. [Text To Video+Audio](#TextToVideoAudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LoadModel'></a>\n",
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install os\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_module_infer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_module\n\u001b[1;32m     12\u001b[0m model_load_paths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoDi_encoders.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoDi_text_diffuser.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoDi_video_diffuser_8frames.pth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m inference_tester \u001b[38;5;241m=\u001b[39m model_module(data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m'\u001b[39m, pth\u001b[38;5;241m=\u001b[39mmodel_load_paths)\n",
      "File \u001b[0;32m~/Desktop/Attri/Research/i-Code/i-Code-V3/core/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_optimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_optimizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mget_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_scheduler\n",
      "File \u001b[0;32m~/Desktop/Attri/Research/i-Code/i-Code-V3/core/models/common/get_model.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from core.common.logger import print_log \u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \\\n\u001b[1;32m      8\u001b[0m     get_total_param, get_total_param_sum, \\\n\u001b[1;32m      9\u001b[0m     get_unit\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# def load_state_dict(net, model_path):\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#     if isinstance(net, dict):\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#         for ni, neti in net.items():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#         torch.save(net.state_dict(), path)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleton\u001b[39m(class_):\n",
      "File \u001b[0;32m~/Desktop/Attri/Research/i-Code/i-Code-V3/core/models/common/utils.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m########\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# unit #\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m########\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msingleton\u001b[39m(class_):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load model from checkpoint.\n",
    "\n",
    "For model inference:\n",
    "The outputs are stored in an array as [number of output modalities, number of samples]\n",
    "If I generate 4 samples of image + caption, the shape would be [2, 4]\n",
    "\"\"\"\n",
    "import os\n",
    "from core.models.model_module_infer import model_module\n",
    "\n",
    "model_load_paths = ['CoDi_encoders.pth', 'CoDi_text_diffuser.pth', 'CoDi_video_diffuser_8frames.pth']\n",
    "inference_tester = model_module(data_dir='../', pth=model_load_paths)\n",
    "inference_tester = inference_tester.cuda()\n",
    "inference_tester = inference_tester.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToImage'></a>\n",
    "# Text To Image\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference_tester' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA beautiful oil painting of a birch tree standing in a spring meadow with pink flowers, a distant mountain towers over the field in the distance. Artwork by Alena Aenami\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Generate image\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43minference_tester\u001b[49m\u001b[38;5;241m.\u001b[39minference(\n\u001b[1;32m      6\u001b[0m                 xtype \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m                 condition \u001b[38;5;241m=\u001b[39m [prompt],\n\u001b[1;32m      8\u001b[0m                 condition_types \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      9\u001b[0m                 n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m     10\u001b[0m                 image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m     11\u001b[0m                 ddim_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     12\u001b[0m images[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inference_tester' is not defined"
     ]
    }
   ],
   "source": [
    "# Give a prompt\n",
    "prompt = \"A beautiful oil painting of a birch tree standing in a spring meadow with pink flowers, a distant mountain towers over the field in the distance. Artwork by Alena Aenami\"\n",
    "\n",
    "# Generate image\n",
    "images = inference_tester.inference(\n",
    "                xtype = ['image'],\n",
    "                condition = [prompt],\n",
    "                condition_types = ['text'],\n",
    "                n_samples = 1, \n",
    "                image_size = 256,\n",
    "                ddim_steps = 50)\n",
    "images[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImageToText'></a>\n",
    "# Image To Text\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load an image input\n",
    "from PIL import Image\n",
    "im = Image.open('./assets/demo_files/house.jpeg').resize((224,224))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = inference_tester.inference(\n",
    "                xtype = ['text'],\n",
    "                condition = [im],\n",
    "                condition_types = ['image'],\n",
    "                n_samples = 4, \n",
    "                ddim_steps = 50,\n",
    "                scale = 7.5,)\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToAudio'></a>\n",
    "# Text To Audio\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give a prompt\n",
    "prompt = 'a train enters station.'\n",
    "\n",
    "# Generate audio\n",
    "audio_wave = inference_tester.inference(\n",
    "                xtype = ['audio'],\n",
    "                condition = [prompt],\n",
    "                condition_types = ['text'],\n",
    "                scale = 7.5,\n",
    "                n_samples = 1, \n",
    "                ddim_steps = 50)[0]\n",
    "\n",
    "# Play the audio\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wave.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AudioToText'></a>\n",
    "# Audio To Text\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "path = './assets/demo_files/train_sound.flac'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * 10.23)]\n",
    "Audio(audio_wavs.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 4\n",
    "text = inference_tester.inference(\n",
    "                xtype = ['text'],\n",
    "                condition = [audio_wavs],\n",
    "                condition_types = ['audio'],\n",
    "                n_samples = n_samples, \n",
    "                ddim_steps = 50,\n",
    "                scale = 7.5)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ImageToAudio'></a>\n",
    "# Image To Audio\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load an image\n",
    "from PIL import Image\n",
    "from core.common.utils import regularize_image\n",
    "im = Image.open('./assets/demo_files/rain_on_tree.jpg')\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate audio\n",
    "audio_wave = inference_tester.inference(\n",
    "                xtype = ['audio'],\n",
    "                condition = [im],\n",
    "                condition_types = ['image'],\n",
    "                scale = 7.5,\n",
    "                n_samples = 1, \n",
    "                ddim_steps = 50)[0]\n",
    "\n",
    "# Play audio\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wave.squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='AudioToImage'></a>\n",
    "# Audio To Image\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input audio andplay\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "pad_time = 10.23\n",
    "\n",
    "path = './assets/demo_files/wind_chimes.wav'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * pad_time)]\n",
    "padding = torch.zeros([int(16000 * pad_time) - audio_wavs.size(0)])\n",
    "audio_wavs = torch.cat([audio_wavs, padding], 0)\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(path, rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate image\n",
    "images = inference_tester.inference(\n",
    "                xtype = ['image'],\n",
    "                condition = [audio_wavs],\n",
    "                condition_types = ['audio'],\n",
    "                scale = 7.5,\n",
    "                image_size = 256,\n",
    "                ddim_steps = 50)\n",
    "images[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToVideo'></a>\n",
    "# Text To Video\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'a beautiful waterfall.'\n",
    "\n",
    "n_samples = 1\n",
    "outputs = inference_tester.inference(\n",
    "                ['video'],\n",
    "                condition = [prompt],\n",
    "                condition_types = ['text'],\n",
    "                n_samples = 1,\n",
    "                image_size = 256,\n",
    "                ddim_steps = 50,\n",
    "                num_frames = 8,\n",
    "                scale = 7.5)\n",
    "\n",
    "video = outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual video as gif\n",
    "from PIL import Image\n",
    "frame_one = video[0]\n",
    "path = \"./generated_text2video.gif\"\n",
    "frame_one.save(path, format=\"GIF\", append_images=video[1:],\n",
    "               save_all=True, duration=2000/len(video), loop=0)\n",
    "\n",
    "from IPython import display \n",
    "from IPython.display import Image\n",
    "Image(data=open(path,'rb').read(), format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextImageAudioToImage'></a>\n",
    "#  Text + Audio To Image\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Audio Inputs\n",
    "import torchaudio\n",
    "import torch\n",
    "from IPython.display import Audio\n",
    "\n",
    "path = './assets/demo_files/sea_waves.wav'\n",
    "\n",
    "audio_wavs, sr = torchaudio.load(path)\n",
    "audio_wavs = torchaudio.functional.resample(waveform=audio_wavs, orig_freq=sr, new_freq=16000).mean(0)[:int(16000 * 10.23)]\n",
    "Audio(audio_wavs.squeeze(), rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'dawn, dawn scenery, sunset, beautiful lighting.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate image\n",
    "# Mix weight here is the weighting ratio of the condition inputs\n",
    "\n",
    "n_samples = 1\n",
    "images = inference_tester.inference(\n",
    "                ['image'],\n",
    "                condition = [audio_wavs, prompt],\n",
    "                condition_types = ['audio', 'text'],\n",
    "                n_samples = n_samples,\n",
    "                image_size = 256,\n",
    "                mix_weight = {'audio': 1, 'text': 2}, )\n",
    "\n",
    "images[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToImageText'></a>\n",
    "#  Text To Image + Text\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'deep diving in coral reef underwater.'\n",
    "\n",
    "outputs = inference_tester.inference(\n",
    "                ['image', 'text'],\n",
    "                condition = [prompt],\n",
    "                condition_types = ['text'],\n",
    "                n_samples = 1,\n",
    "                image_size = 256)\n",
    "\n",
    "image, text = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TextToVideoAudio'></a>\n",
    "#  Text To Video + Audio\n",
    "\n",
    "### [Back to Menu](#ContentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give A Prompt\n",
    "prompt = 'walking inside a beautiful forest, nature, birds.'\n",
    "\n",
    "\n",
    "n_samples = 1\n",
    "outputs = inference_tester.inference(\n",
    "                ['video', 'audio'],\n",
    "                condition = [prompt],\n",
    "                condition_types = ['text'],\n",
    "                n_samples = 1,\n",
    "                image_size = 256,\n",
    "                ddim_steps = 50,\n",
    "                num_frames = 8,\n",
    "                scale = 7.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video, audio_wave = outputs\n",
    "\n",
    "from IPython.display import Audio\n",
    "Audio(audio_wave[0].squeeze(), rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual video as gif\n",
    "video = video[0]\n",
    "from PIL import Image\n",
    "frame_one = video[0]\n",
    "path = \"./generated_video.gif\"\n",
    "frame_one.save(path, format=\"GIF\", append_images=video[1:],\n",
    "               save_all=True, duration=2000/len(video), loop=0)\n",
    "\n",
    "from IPython import display \n",
    "from IPython.display import Image\n",
    "Image(data=open(path,'rb').read(), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
